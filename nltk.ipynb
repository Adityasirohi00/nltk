{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161d2454-7bde-454b-93b9-126e4fed15ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae27bef9-8b84-41e3-89be-5aa18331a517",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac9b73b-812c-4bb5-89bc-dfb17292fcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')  # Tokenization\n",
    "nltk.download('averaged_perceptron_tagger')  # POS Tagging\n",
    "nltk.download('stopwords')  # Stop words\n",
    "nltk.download('wordnet')  # Lemmatization\n",
    "nltk.download('vader_lexicon')  # Sentiment Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed33b70-401e-4552-8c10-49508295b96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "text='This is an example sentence'\n",
    "tokens=word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50de69e1-ab48-4257-b086-63b5026fa4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "text=\"Hello! How are you? This is an example.\"\n",
    "sentences=sent_tokenize(text)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b736c51-0598-4e81-b801-777989b74311",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words=set(stopwords.words('english'))\n",
    "tokens=word_tokenize(\"This is a sample sentence, showing off the stop words filtration.\")\n",
    "filtered_tokens=[word for word in tokens if word.lower() not in stop_words]\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bb4a7a-2abb-457e-ad01-32b6af296868",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps=PorterStemmer()\n",
    "words=[\"running\",\"ran\",\"runs\"]\n",
    "stemmed_words=[ps.stem(word) for word in words]\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e153369-0a56-44db-85aa-83c98b4c5e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words = [\"running\", \"ran\", \"runs\"]\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "print(lemmatized_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dc7239-6a56-47ba-9af6-c83779bac429",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "tokens=word_tokenize(\"This is a sample sentence.\")\n",
    "pos_tags=pos_tag(tokens)\n",
    "print(pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abe60dc-d83a-488f-9c38-ff35b5fc493c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import  ne_chunk\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "# Tokenize the text into words\n",
    "words = word_tokenize(\"Barack Obama was born in Hawaii.\")\n",
    "\n",
    "# Perform POS tagging\n",
    "pos_tags = pos_tag(words)\n",
    "\n",
    "# Perform NER using ne_chunk\n",
    "named_entities = ne_chunk(pos_tags)\n",
    "\n",
    "print(named_entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e5c8ef-afba-47e4-8bf8-33531869f5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Initialize the VADER sentiment analyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Sentence to analyze\n",
    "sentence = \"this is a great movie!\"\n",
    "\n",
    "# Perform sentiment analysis on the sentence\n",
    "scores = sid.polarity_scores(sentence)\n",
    "\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f519ee78-36e1-4b25-ad3b-e6ec452b359a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"I love natural language processing. Processing is fun.\"\n",
    "words = word_tokenize(text)\n",
    "fdist = FreqDist(words)\n",
    "\n",
    "print(words)\n",
    "print(fdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f944d9-d5df-4ec3-bcb0-58d13eadde92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"I love natural language processing.\"\n",
    "words = word_tokenize(text)\n",
    "bigrams = list(ngrams(words, 2))  # Bigrams\n",
    "trigrams = list(ngrams(words, 3))  # Trigrams\n",
    "print(words)\n",
    "print(bigrams)\n",
    "print(trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e00250-570c-416d-bf84-52f8463157f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Download NLTK resources (if not already downloaded)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Sample corpus of documents\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "\n",
    "# Initialize NLTK's English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Tokenization using NLTK\n",
    "    tokens = nltk.word_tokenize(text.lower())  # Convert to lowercase\n",
    "    # Remove stopwords and punctuation\n",
    "    tokens = [token for token in tokens if token.isalnum() and token not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Preprocess the corpus\n",
    "preprocessed_corpus = [preprocess_text(doc) for doc in corpus]\n",
    "\n",
    "# Initialize TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "\n",
    "# Fit and transform the corpus\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(preprocessed_corpus)\n",
    "\n",
    "# Print the TF-IDF matrix (sparse format)\n",
    "print(tfidf_matrix)\n",
    "\n",
    "# Convert TF-IDF matrix to dense array and print (for demonstration purposes)\n",
    "print(tfidf_matrix.toarray())\n",
    "\n",
    "# Print the feature names (terms)\n",
    "print(tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Example usage of TF-IDF features\n",
    "# Accessing TF-IDF values for the first document\n",
    "print(\"TF-IDF values for the first document:\")\n",
    "for word, index in tfidf_vectorizer.vocabulary_.items():\n",
    "    print(f\"{word}: {tfidf_matrix[0, index]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a0b94d-a673-4d23-b596-24195c0a5dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# Sample document\n",
    "document = \"This is a sample document. We will calculate term frequencies for each word in this document.\"\n",
    "\n",
    "# Tokenize the document using NLTK\n",
    "tokens = word_tokenize(document.lower())  # Convert to lowercase for case-insensitivity\n",
    "\n",
    "# Calculate frequency distribution of words\n",
    "freq_dist = FreqDist(tokens)\n",
    "\n",
    "# Print the term frequency of each word\n",
    "print(\"Term Frequencies:\")\n",
    "for word, frequency in freq_dist.items():\n",
    "    print(f\"{word}: {frequency}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fae1bd-6736-47ad-a260-d6742d32dd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import math\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\",\n",
    "]\n",
    "\n",
    "# Function to calculate IDF for each term\n",
    "def calculate_idf(documents):\n",
    "    # Tokenize each document\n",
    "    tokenized_docs = [word_tokenize(doc.lower()) for doc in documents]\n",
    "    \n",
    "    # Set of unique words across all documents\n",
    "    unique_words = set([word for doc in tokenized_docs for word in doc])\n",
    "    \n",
    "    # Calculate document frequency (DF) for each term\n",
    "    df = defaultdict(int)\n",
    "    for word in unique_words:\n",
    "        for doc in tokenized_docs:\n",
    "            if word in doc:\n",
    "                df[word] += 1\n",
    "    \n",
    "    # Calculate IDF for each term\n",
    "    idf = {}\n",
    "    total_docs = len(documents)\n",
    "    for word, freq in df.items():\n",
    "        idf[word] = math.log(total_docs / (freq + 1))  # Add 1 to avoid division by zero and smoothing\n",
    "    \n",
    "    return idf\n",
    "\n",
    "# Calculate IDF for the sample documents\n",
    "idf_scores = calculate_idf(documents)\n",
    "\n",
    "# Print IDF scores for each term\n",
    "print(\"Inverse Document Frequency (IDF) Scores:\")\n",
    "for word, score in idf_scores.items():\n",
    "    print(f\"{word}: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af63f76-30c9-40af-bc60-8d700100b825",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import math\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\",\n",
    "]\n",
    "\n",
    "# Initialize NLTK's English stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Tokenization and preprocessing using NLTK\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())  # Convert to lowercase\n",
    "    tokens = [token for token in tokens if token.isalnum() and token not in stop_words]  # Remove stopwords and non-alphanumeric tokens\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Preprocess the documents\n",
    "preprocessed_documents = [preprocess_text(doc) for doc in documents]\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the preprocessed documents\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(preprocessed_documents)\n",
    "\n",
    "# Print TF-IDF weights\n",
    "print(\"TF-IDF Weights:\")\n",
    "for i, doc in enumerate(documents):\n",
    "    print(f\"Document {i+1}: {doc}\")\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "    for j, term in enumerate(feature_names):\n",
    "        tfidf_weight = tfidf_matrix[i, j]\n",
    "        if tfidf_weight > 0:\n",
    "            print(f\"  {term}: {tfidf_weight:.4f}\")\n",
    "\n",
    "# Example usage of TF-IDF features\n",
    "# Transform a new document\n",
    "new_document = \"This document is different from the others.\"\n",
    "tfidf_vector = tfidf_vectorizer.transform([preprocess_text(new_document)])\n",
    "print(\"\\nTF-IDF Weights for New Document:\")\n",
    "for j, term in enumerate(feature_names):\n",
    "    tfidf_weight = tfidf_vector[0, j]\n",
    "    if tfidf_weight > 0:\n",
    "        print(f\"  {term}: {tfidf_weight:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a040adcc-a635-411e-b748-47e1efc1b789",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import math\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\",\n",
    "]\n",
    "\n",
    "# Initialize NLTK's English stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Tokenization and preprocessing using NLTK\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())  # Convert to lowercase\n",
    "    tokens = [token for token in tokens if token.isalnum() and token not in stop_words]  # Remove stopwords and non-alphanumeric tokens\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Preprocess the documents\n",
    "preprocessed_documents = [preprocess_text(doc) for doc in documents]\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the preprocessed documents\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(preprocessed_documents)\n",
    "\n",
    "# Print TF-IDF weights\n",
    "print(\"TF-IDF Weights:\")\n",
    "for i, doc in enumerate(documents):\n",
    "    print(f\"Document {i+1}: {doc}\")\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "    for j, term in enumerate(feature_names):\n",
    "        tfidf_weight = tfidf_matrix[i, j]\n",
    "        if tfidf_weight > 0:\n",
    "            print(f\"  {term}: {tfidf_weight:.4f}\")\n",
    "\n",
    "# Example usage of TF-IDF features\n",
    "# Transform a new document\n",
    "new_document = \"This document is different from the others.\"\n",
    "tfidf_vector = tfidf_vectorizer.transform([preprocess_text(new_document)])\n",
    "print(\"\\nTF-IDF Weights for New Document:\")\n",
    "for j, term in enumerate(feature_names):\n",
    "    tfidf_weight = tfidf_vector[0, j]\n",
    "    if tfidf_weight > 0:\n",
    "        print(f\"  {term}: {tfidf_weight:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab332b2-b0d3-4859-850e-b9a1c15aa786",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Step 1: Prepare the corpus\n",
    "corpus = [\n",
    "    \"I love machine learning\",\n",
    "    \"Machine learning is fun\",\n",
    "    \"Natural language processing includes machine learning\"\n",
    "]\n",
    "\n",
    "# Step 2: Create the CountVectorizer without removing stop words\n",
    "vectorizer = CountVectorizer(stop_words=None)\n",
    "\n",
    "# Step 3: Fit and transform the corpus\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Step 4: Get feature names\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Step 5: Convert to array\n",
    "X_array = X.toarray()\n",
    "\n",
    "# Display the results\n",
    "print(\"Feature Names:\")\n",
    "print(feature_names)\n",
    "print(\"\\nBag of Words Representation:\")\n",
    "print(X_array)\n",
    "\n",
    "# Print the column names with the corresponding bag of words matrix\n",
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "df = pd.DataFrame(X_array, columns=feature_names)\n",
    "\n",
    "print(\"\\nBag of Words Representation with Column Names:\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475e9a2c-111f-45d2-901d-96cba591ba52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Step 1: Prepare the corpus\n",
    "corpus = [\n",
    "    \"I love machine learning\",\n",
    "    \"Machine learning is fun\",\n",
    "    \"Natural language processing includes machine learning\"\n",
    "]\n",
    "\n",
    "# Step 2: Create the TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words=None)\n",
    "\n",
    "# Step 3: Fit and transform the corpus\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Step 4: Get feature names\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Step 5: Convert to array\n",
    "X_array = X.toarray()\n",
    "\n",
    "# Display the results\n",
    "print(\"Feature Names:\")\n",
    "print(feature_names)\n",
    "print(\"\\nTF-IDF Representation:\")\n",
    "print(X_array)\n",
    "\n",
    "# Print the column names with the corresponding TF-IDF matrix\n",
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "df = pd.DataFrame(X_array, columns=feature_names)\n",
    "\n",
    "print(\"\\nTF-IDF Representation with Column Names:\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904cab4b-e740-41e2-8655-5552ffc22771",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Step 1: Prepare the corpus\n",
    "corpus = [\n",
    "    [\"I\", \"love\", \"machine\", \"learning\"],\n",
    "    [\"Machine\", \"learning\", \"is\", \"fun\"],\n",
    "    [\"Natural\", \"language\", \"processing\", \"includes\", \"machine\", \"learning\"]\n",
    "]\n",
    "\n",
    "# Step 2: Create and train the Word2Vec model\n",
    "model = Word2Vec(sentences=corpus, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Step 3: Get the vector for a specific word\n",
    "vector = model.wv['machine']\n",
    "\n",
    "# Step 4: Find similar words\n",
    "similar_words = model.wv.most_similar('machine')\n",
    "\n",
    "# Display the results\n",
    "print(\"Vector for 'machine':\")\n",
    "print(vector)\n",
    "print(\"\\nWords similar to 'machine':\")\n",
    "print(similar_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7e650e-b77b-4e35-8481-bf48effc7adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Step 1: Prepare the corpus\n",
    "corpus = [\n",
    "    [\"i\", \"love\", \"machine\", \"learning\"],\n",
    "    [\"machine\", \"learning\", \"is\", \"fun\"],\n",
    "    [\"natural\", \"language\", \"processing\", \"includes\", \"machine\", \"learning\"]\n",
    "]\n",
    "\n",
    "# Step 2: Create and train the CBOW model\n",
    "model = Word2Vec(sentences=corpus, vector_size=100, window=5, min_count=1, sg=0, workers=4)\n",
    "\n",
    "# Step 3: Get vector for a specific word\n",
    "vector_love = model.wv['love']\n",
    "\n",
    "# Step 4: Find similar words\n",
    "similar_words = model.wv.most_similar('love')\n",
    "\n",
    "# Display the results\n",
    "print(\"Vector for 'love':\")\n",
    "print(vector_love)\n",
    "print(\"\\nWords similar to 'love':\")\n",
    "print(similar_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558f9272-c2c8-4200-989e-7567e038c905",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Example documents\n",
    "documents = [\n",
    "    \"I love machine learning and data science.\",\n",
    "    \"Machine learning is fascinating and enjoyable.\",\n",
    "    \"Natural language processing is a subfield of artificial intelligence.\",\n",
    "    \"I enjoy studying data science and machine learning.\"\n",
    "]\n",
    "\n",
    "# Step 1: Convert text documents to TF-IDF vectors\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Step 2: Compute cosine similarity matrix\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Display cosine similarity matrix\n",
    "print(\"Cosine Similarity Matrix:\")\n",
    "print(cosine_sim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e10503-0f65-43ac-a96b-b8fbd6f8d83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Example documents\n",
    "documents = [\n",
    "    \"I love machine learning and data science.\",\n",
    "    \"Machine learning is fascinating and enjoyable.\"\n",
    "]\n",
    "\n",
    "# Step 1: Convert text documents to TF-IDF vectors\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Step 2: Compute cosine similarity\n",
    "cosine_sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
    "\n",
    "# Display the cosine similarity\n",
    "print(\"Cosine Similarity between Document 1 and Document 2:\")\n",
    "print(cosine_sim[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f85663-0838-49d3-8fdb-435c33703871",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "# Example documents\n",
    "documents = [\n",
    "    \"I love machine learning and data science.\",\n",
    "    \"Machine learning is fascinating and enjoyable.\",\n",
    "    \"Natural language processing is a subfield of artificial intelligence.\",\n",
    "    \"I enjoy studying data science and machine learning.\"\n",
    "]\n",
    "\n",
    "# Step 1: Convert text documents to TF-IDF vectors\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Step 2: Perform K-means clustering\n",
    "num_clusters = 2  # Number of clusters\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n",
    "kmeans.fit(tfidf_matrix)\n",
    "\n",
    "# Step 3: Print cluster labels for each document\n",
    "labels = kmeans.labels_\n",
    "print(\"Cluster Labels:\")\n",
    "for i, label in enumerate(labels):\n",
    "    print(f\"Document {i}: Cluster {label}\")\n",
    "\n",
    "# Optional: Display the cluster centroids\n",
    "print(\"\\nCluster Centroids:\")\n",
    "centroids = kmeans.cluster_centers_\n",
    "print(centroids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9069c8a5-854d-433d-b92a-b541559a075e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "# Sample texts\n",
    "documents = [\"Text similarity measures are useful.\",\n",
    "              \"Comparing the similarity between texts can be done using cosine similarity.\"]\n",
    "\n",
    "# Vectorize the texts\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(documents).toarray()\n",
    "\n",
    "# Compute Euclidean distance\n",
    "distance = euclidean(vectors[0], vectors[1])\n",
    "similarity = 1 / (1 + distance)  # Convert to similarity\n",
    "print(similarity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aca1193-d068-45c6-b852-47721dfd2366",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Sample dataset\n",
    "texts = [\n",
    "    \"I love programming.\",\n",
    "    \"Python is a great language.\",\n",
    "    \"I hate bugs in my code.\",\n",
    "    \"Java is also a good language.\",\n",
    "    \"Debugging is fun when you solve the problem.\",\n",
    "]\n",
    "labels = [\"positive\", \"positive\", \"negative\", \"positive\", \"positive\"]\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "# Feature extraction\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_features = vectorizer.fit_transform(X_train)\n",
    "X_test_features = vectorizer.transform(X_test)\n",
    "\n",
    "# Train a classifier\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train_features, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = classifier.predict(X_test_features)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dbfd15-531f-48a5-a824-6f18dd52d928",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim import corpora, models\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"I love programming in Python.\",\n",
    "    \"Python is a great language for data science.\",\n",
    "    \"Machine learning and data analysis are exciting fields.\",\n",
    "    \"Gensim is useful for topic modeling and document similarity.\",\n",
    "    \"Data science involves programming, statistics, and machine learning.\",\n",
    "    \"Topic modeling helps in uncovering hidden themes in large corpora.\"\n",
    "]\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Tokenization and stopword removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [token for token in tokens if token.isalpha()]  # Remove punctuation\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "processed_docs = [preprocess_text(doc) for doc in documents]\n",
    "\n",
    "# Create dictionary and corpus\n",
    "dictionary = corpora.Dictionary(processed_docs)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "\n",
    "# Train LDA model\n",
    "lda_model = models.LdaModel(corpus, num_topics=2, id2word=dictionary, passes=15)\n",
    "\n",
    "# Print topics\n",
    "topics = lda_model.print_topics(num_words=4)\n",
    "for topic in topics:\n",
    "    print(topic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3f0b3d-c5d7-466b-832d-d52b4e12d157",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.chat.util import Chat, reflections\n",
    "\n",
    "# Define pairs of patterns and responses\n",
    "pairs = [\n",
    "    (r'Hi|Hello|Hey', ['Hello! How can I help you today?', 'Hi there!']),\n",
    "    (r'What is your name?', ['I am a chatbot created by ChatGPT.']),\n",
    "    (r'I need help with (.*)', ['Sure, I can help with %1.']),\n",
    "    (r'Bye|Goodbye', ['Goodbye! Have a great day!']),\n",
    "]\n",
    "\n",
    "# Create a chatbot\n",
    "chatbot = Chat(pairs, reflections)\n",
    "\n",
    "# Chat with the user\n",
    "def chat():\n",
    "    print(\"Hello! I am a simple rule-based chatbot. Type 'Bye' to exit.\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() in ['bye', 'goodbye']:\n",
    "            print(\"Chatbot: Goodbye! Have a great day!\")\n",
    "            break\n",
    "        response = chatbot.respond(user_input)\n",
    "        print(f\"Chatbot: {response}\")\n",
    "\n",
    "# Start chatting\n",
    "chat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ac0a22-efdb-4c3d-ad7e-566de2b1681b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
